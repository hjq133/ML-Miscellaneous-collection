{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意，这是分类问题的梯度提升树\n",
    "# 回归问题的梯度提升树是一个残差学习过程，见统计学习方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    data_arr = []\n",
    "    label_arr = []\n",
    "    fr = open(file_name)\n",
    "    \n",
    "    for line in fr.readlines():\n",
    "        cur_line = line.strip().split(',')\n",
    "        # 将数据二值化进行处理，大于128的转成1，小雨的转成0\n",
    "        data_arr.append([int(int(num) > 128) for num in curLine[1:]])\n",
    "        \n",
    "        if int(cur_line[0]) == 0:\n",
    "            label_arr.append(1)\n",
    "        else:\n",
    "            label_arr.append(-1)\n",
    "    return data_arr, label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_e_Gx(train_data, train_label, n, div, rule, D):\n",
    "    #初始化分类误差率为0\n",
    "    e = 0\n",
    "    #将训练数据矩阵中特征为n的那一列单独剥出来做成数组。因为其他元素我们并不需要，\n",
    "    #直接对庞大的训练集进行操作的话会很慢\n",
    "    x = train_data[:, n]\n",
    "    #同样将标签也转换成数组格式，x和y的转换只是单纯为了提高运行速度\n",
    "    #测试过相对直接操作而言性能提升很大\n",
    "    y = train_label\n",
    "    predict = []\n",
    "\n",
    "    #依据小于和大于的标签依据实际情况会不同，在这里直接进行设置\n",
    "    if rule == 'LisOne':    L = 1; H = -1\n",
    "    else:                   L = -1; H = 1\n",
    "\n",
    "    #遍历所有样本的特征m\n",
    "    for i in range(train_data.shape[0]):\n",
    "        if x[i] < div:\n",
    "            #如果小于划分点，则预测为L\n",
    "            #如果设置小于div为1，那么L就是1，\n",
    "            #如果设置小于div为-1，L就是-1\n",
    "            predict.append(L)\n",
    "            #如果预测错误，分类错误率要加上该分错的样本的权值（8.1式）\n",
    "            if y[i] != L: e += D[i]\n",
    "        elif x[i] >= div:\n",
    "            #与上面思想一样\n",
    "            predict.append(H)\n",
    "            if y[i] != H: e += D[i]\n",
    "    #返回预测结果和分类错误率e\n",
    "    #预测结果其实是为了后面做准备的，在算法8.1第四步式8.4中exp内部有个Gx，要用在那个地方\n",
    "    #以此来更新新的D\n",
    "    return np.array(predict), e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSingleBoostingTree(train_data, train_label, D):\n",
    "    m, n = np.shape(train_data)\n",
    "    single_boost_tree = {}\n",
    "    # 初始化分类误差率\n",
    "    single_boost_tree['e'] = 1\n",
    "    #对每一个特征进行遍历，寻找用于划分的最合适的特征\n",
    "    for i in range(n):\n",
    "        #因为特征已经经过二值化，只能为0和1，因此分切分时分为-0.5， 0.5， 1.5三挡进行切割\n",
    "        for div in [-0.5, 0.5, 1.5]:\n",
    "            #在单个特征内对正反例进行划分时，有两种情况：\n",
    "            #可能是小于某值的为1，大于某值得为-1，也可能小于某值得是-1，反之为1\n",
    "            #因此在寻找最佳提升树的同时对于两种情况也需要遍历运行\n",
    "            #LisOne：Low is one：小于某值得是1\n",
    "            #HisOne：High is one：大于某值得是1\n",
    "            for rule in ['LisOne', 'HisOne']:\n",
    "                #按照第i个特征，以值div进行切割，进行当前设置得到的预测和分类错误率\n",
    "                Gx, e = calc_e_Gx(train_data, train_label, i, div, rule, D)\n",
    "                #如果分类错误率e小于当前最小的e，那么将它作为最小的分类错误率保存\n",
    "                if e < single_boost_tree['e']:\n",
    "                    single_boost_tree['e'] = e\n",
    "                    #同时也需要存储最优划分点、划分规则、预测结果、特征索引\n",
    "                    #以便进行D更新和后续预测使用\n",
    "                    single_boost_tree['div'] = div\n",
    "                    single_boost_tree['rule'] = rule\n",
    "                    single_boost_tree['Gx'] = Gx\n",
    "                    single_boost_tree['feature'] = i\n",
    "    #返回单层的提升树\n",
    "    return single_boost_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBosstingTree(train_data, train_label, tree_num = 50):\n",
    "    train_data = np.array(train_data)\n",
    "    train_label = np.array(train_label)\n",
    "    \n",
    "    # 最终预测结果列表\n",
    "    finall_predict = [0] * len(train_label)\n",
    "    \n",
    "    m, n = np.shape(train_data)\n",
    "    D = [1 / m] * m  # D为训练集数据的权重, 这里进行初始化\n",
    "    tree = []\n",
    "    \n",
    "    # 循环建立提升树\n",
    "    for i in range(tree_num):\n",
    "        # 建立单个树，并在当前权重D下计算分类误差率e\n",
    "        cur_tree = createSingleBoostingTree(train_data, train_label, D)\n",
    "        # 根据分类误差率计算当前树的权重alpha\n",
    "        alpha = 1/2 * np.log((1 - cur_tree['e']) / cur_tree['e'])\n",
    "        # Gx就是当前的树\n",
    "        Gx = cur_tree['Gx']\n",
    "        # 更新训练数据集的权值分布\n",
    "        D = np.multiply(D, np.exp(-1 * alpha * np.multiply(train_label, Gx))) / sum(D)\n",
    "        cur_tree['alpha'] = alpha\n",
    "        tree.append(cur_tree)\n",
    "        \n",
    "        finall_predict += alpha * Gx  # 前向分布，加法模型\n",
    "        error = sum([1 for i in range(len(train_data)) if np.sign(finall_predict[i]) != train_label[i]])\n",
    "        finall_error = error / len(train_data)\n",
    "        if finall_error == 0:\n",
    "            return tree\n",
    "        print('iter:%d:%d, sigle error:%.4f, finall error:%.4f'%(i, tree_num, cur_tree['e'], finall_error))\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
